# MicrosoftMalware
Predicting weak links in Microsoft systems for malware attacks

## 1.	Title: Microsoft Malware Prediction 

## 2.	Introduction and Statement of the Problem

  •	Now a days detecting malware has been difficult and finding the potential malware has been even more. These malwares are worsening day by day and troubling not only the personal/organization systems but all the cyberspace.

  •	In order to protect the systems for malware and there by finding weak links in system architecture and to secure, this project is taken up.
  
  •	Objective of this project is by applying advance machine learning models to find hidden patterns. Constructing infrastructure to collect data,clean data, build and deploy models.

## 3.	Review of Literature

In this current Internet age malwares like virus ransomware and Trojan horse are serious an alarming security threat to the Internet users, organizations and private users. To protect users from these threat’s anti malware software products from different companies provide a major defense against the malwares. Unfortunately, the number of new malware samples has explosively increased. 

In order to keep on fighting the increase of malware samples there is an important to develop effective methods. An efficient malware detection from the real and large daily sample collection. Following the current trend and one of the most common approaches in literature using machine learning techniques it automatically learns models and patterns behind the complexity and develop functionalities to handle with malware evolution.

This project aims to provide an overview on machine learning models how they can be used in context of malware analysis in windows environment. The objective is to use multiple machine learning models especially tree models to classify the malware prediction using the given data from Kaggle. and initially with the domain knowledge removing multiple features the are insignificant for the analysis and by using statistics inferences removing few more features and thereby filling out empty relations and then training the model with the data to predict the weak link in the system that causes malware. 

## 4.	Objectives of the Study

  To predict a Windows machine’s probability of getting infected by various families of malware, based on different properties of that machine. The telemetry data containing these properties and the machine infections was generated by combining heartbeat and threat reports collected by Microsoft's endpoint protection solution, Windows Defender.
  
## 5.	Data Collection (Give the link to the files, or upload your files if they are not accessible online)
Data acquisition:

Microsoft is providing Kaggle’s with an unprecedented malware dataset to encourage open-source progress on effective techniques for predicting malware occurrences.

Source: https://www.kaggle.com/c/microsoft-malware-prediction/data

## 6.	Exploratory data analysis (EDA) and Hypotheses for the Study

Exploratory data analysis:

Analyzing data set to summarize the main characters. Using correlation, heatmaps doing the feature selections and grouping the observations. Plotting bar plots and histograms trends gives further information on data distributions.

### correlation Plot 1 
  ![Correlation plot 1 ](https://github.com/TRKSasank/MicrosoftMalware/blob/main/Images/Corr%20plot%201.png)
  
### correlation Plot 2
  ![Correlation plot 2 ](https://github.com/TRKSasank/MicrosoftMalware/blob/main/Images/Corr%20plot%202.png)

### correlation Plot 3 
  ![Correlation plot 3 ](https://github.com/TRKSasank/MicrosoftMalware/blob/main/Images/Corr%20plot%203.png)
  

  ![target variable count](https://github.com/TRKSasank/MicrosoftMalware/blob/main/Images/Target%20Count.png)
  
  ![Activation Channel](https://github.com/TRKSasank/MicrosoftMalware/blob/main/Images/Activation%20Channel%20Count.png)
  
  ![Census Edition](https://github.com/TRKSasank/MicrosoftMalware/blob/main/Images/Census%20OS%20Edition.png)
  
  ![Antivirus Sign Version](https://github.com/TRKSasank/MicrosoftMalware/blob/main/Images/AV%20sign%20Version.png)
  
  ![Gamers ](https://github.com/TRKSasank/MicrosoftMalware/blob/main/Images/Gamer.png)
  
  ![Smart Screen Enablement](https://github.com/TRKSasank/MicrosoftMalware/blob/main/Images/Smart%20Screen%20.png)
  
  ![OS Install Type ](https://github.com/TRKSasank/MicrosoftMalware/blob/main/Images/OSInstallType.png)
  
  ![Dashboard](https://github.com/TRKSasank/MicrosoftMalware/blob/main/Images/Final.png)
  
## 7.	Data Analytics

### Feature Selection:

### Domain Specific insignificant feature removal:

### Duplicate observation Removal:

### Excluding (NULL) features:

### Data Cleaning;

### Skewed Data Excluding:

### Significant features selection and feature matrix building:

Narrowed down to 35 features and built a sparse matrix, which handle matrix with zeros values called non dense matrix and converts to dense matrix.


### Data analysis:

With more the 4GB of training data and 83 features, preliminary analysis on data is required by inspecting, transforming and cleaning in process of discovering useful information and derive conclusion on population. 

### Analytical models used:
### Random Forest:

Ensemble learning method for classification tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set. Random forests generally outperform decision trees, but their accuracy is lower than gradient boosted trees. However, data characteristics can affect their performance. 

### AdaBoost:

AdaBoost training process selects only those features known to improve the predictive power of the model, reducing dimensionality and potentially improving execution time as irrelevant features don't need to be computed.

### GBC:
It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.

###  XGBoost:
Salient features of XGBoost which make it different from other gradient boosting algorithms include: 

Clever penalization of trees

Proportional shrinking of leaf nodes

Extra randomization parameter

###  Light Gradient Boost Machine:

It is based on decision tree algorithms and used for ranking, classification and other machine learning tasks. The development focus is on performance and scalability.

Sources: Wikipidea: 
  
  

