import pickle 
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import plot_roc_curve
import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder
import numpy as np
from sklearn.preprocessing import Normalizer
from scipy.sparse import vstack, csr_matrix, save_npz, load_npz



train_df_final = load_npz('train_final.npz')
Y_Train        = np.load('Train_array_Y.npy')

X_train = train_df_final[:1000000,:].astype('float32')
y_train =  Y_Train[:1000000].astype('float32')

X_test = train_df_final[800000:830000,:].astype('float32')
y_test = Y_Train[800000:830000].astype('float32')

ax = plt.gca()

randomforrest = pickle.load(open('modelRandomForest.pkl', 'rb'))
randomforrest_disp = plot_roc_curve(randomforrest, X_test, y_test)
#plt.show()

GBC = pickle.load(open('model3.pkl', 'rb'))
GBC_disp = plot_roc_curve(GBC, X_test, y_test,ax=ax)
randomforrest_disp.plot(ax=ax, alpha=0.8)
plt.show()

modelAdaBoost_pkl = pickle.load(open('modelAdaBoost.pkl', 'rb'))
modelAdaBoost_disp = plot_roc_curve(modelAdaBoost_pkl, X_test, y_test,ax=ax)
plt.show()

model_XBG_pkl = pickle.load(open('model_XBG.pkl', 'rb'))
XGB_disp = plot_roc_curve(model_LBG_pkl, X_test, y_test,ax=ax)
plt.show()


model_LBG_pkl = pickle.load(open('model_LBG.pkl', 'rb'))
LGB_disp = plot_roc_curve(model_LBG_pkl, X_test, y_test,ax=ax)
plt.show()

ax = plt.gca()
randomforrest_disp = plot_roc_curve(randomforrest, X_test, y_test,ax=ax)
GBC_disp = plot_roc_curve(GBC, X_test, y_test,ax=ax)
modelAdaBoost_disp = plot_roc_curve(modelAdaBoost_pkl, X_test, y_test,ax=ax)
XGB_disp = plot_roc_curve(model_XBG_pkl, X_test, y_test,ax=ax)
LGB_disp = plot_roc_curve(model_LBG_pkl, X_test, y_test,ax=ax)
randomforrest_disp.plot(ax=ax, alpha=0.8)
plt.show()

from sklearn.metrics import log_loss
print('Random Forest Log Loss       ',log_loss(y_train,randomforrest.predict_proba(X_train)))
print('Gradient Boost Log Loss      ',log_loss(y_train,GBC.predict_proba(X_train)))
print('Ada Boost Log Loss           ',log_loss(y_train,modelAdaBoost_pkl.predict_proba(X_train)))
print('XGBoost Log Loss             ',log_loss(y_train,model_XBG_pkl.predict_proba(X_train)))
print('LGBoost Log Loss',log_loss(y_train,model_LBG_pkl.predict_proba(X_train)))


from sklearn.metrics import confusion_matrix
print('Random Forest -- ',confusion_matrix(y_train, randomforrest.predict(X_train) ))
print('Gradient Boost-- ',confusion_matrix(y_train, GBC.predict(X_train) ))
print('Ada Boost  -- ',confusion_matrix(y_train, modelAdaBoost_pkl.predict(X_train) ))
print('XGBoost Forest -- ',confusion_matrix(y_train, model_XBG_pkl.predict(X_train) ))
print('LGBoost Forest -- ',confusion_matrix(y_train, model_LBG_pkl.predict(X_train) ))

from sklearn.metrics import f1_score  
print('Random Forest --  ',f1_score(y_train, randomforrest.predict(X_train), average='macro'))
print('Gradient Boost--  ',f1_score(y_train, GBC.predict(X_train), average='macro'))
print('Ada Boost  --     ',f1_score(y_train, modelAdaBoost_pkl.predict(X_train), average='macro'))
print('XGBoost Forest -- ',f1_score(y_train, model_XBG_pkl.predict(X_train), average='macro'))
print('LGBoost Forest -- ',f1_score(y_train, model_LBG_pkl.predict(X_train), average='macro'))